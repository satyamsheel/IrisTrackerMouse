{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data) :\n",
    "    Y = data[[\"x_coord\", \"y_coord\"]].values\n",
    "    X = data[[\"left_ear\", \"right_ear\", \"left_pupil\", \"right_pupil\"]]\n",
    "    for i in range(0, 6) :\n",
    "#         if i != 0 and i != 3:\n",
    "#             continue\n",
    "        right_name = \"right_pupil_\" + str(i + 1)\n",
    "        left_name = \"left_pupil_\" + str(i + 1)\n",
    "        X[right_name] = X[\"right_pupil\"].apply(lambda x : 1 / x[i])\n",
    "        X[left_name] = X[\"left_pupil\"].apply(lambda x : 1 / x[i])\n",
    "    X.drop([\"left_pupil\", \"right_pupil\"], axis = 1, inplace = True)\n",
    "    ret_X = X.values\n",
    "    return X, X.columns, ret_X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(root_path) :\n",
    "    data = None\n",
    "    for folder in os.listdir(root_path) :\n",
    "        if folder.find(\"Images\") != -1 :\n",
    "            df = pd.read_json(root_path + folder + \"/data.json\")\n",
    "            if data is None :\n",
    "                data = df\n",
    "            else :\n",
    "                data = pd.concat([data, df])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               1920      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 78,948\n",
      "Trainable params: 78,948\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(14,)))\n",
    "model.add(Dense(14, activation = \"relu\"))#, kernel_regularizer = l2(0.05)))\n",
    "model.add(Dense(128, activation = \"relu\", kernel_regularizer = l2(0.03)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation = \"relu\"))#, kernel_regularizer = l2(0.03)))\n",
    "model.add(Dense(128, activation = \"relu\"))#, kernel_regularizer = l2(0.03)))\n",
    "model.add(Dense(64, activation = \"relu\", kernel_regularizer = l2(0.05)))\n",
    "model.add(Dense(32, activation = \"relu\"))#, kernel_regularizer = l2(0.05)))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation = \"relu\"))#, kernel_regularizer = l2(0.05)))\n",
    "model.add(Dense(2, activation = \"relu\"))#, kernel_regularizer = l2(0.05)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.009)\n",
    "model.compile(optimizer= adam, loss=\"mse\", metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-707217439ab7>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[right_name] = X[\"right_pupil\"].apply(lambda x : 1 / x[i])\n",
      "<ipython-input-11-707217439ab7>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[left_name] = X[\"left_pupil\"].apply(lambda x : 1 / x[i])\n"
     ]
    }
   ],
   "source": [
    "#train data\n",
    "data = read_data(\"Preprocessed Output/\")\n",
    "data, train_columns, X_train, Y_train = data_preprocessing(data)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, Y_train = shuffle(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_ear</th>\n",
       "      <th>right_ear</th>\n",
       "      <th>right_pupil_1</th>\n",
       "      <th>left_pupil_1</th>\n",
       "      <th>right_pupil_2</th>\n",
       "      <th>left_pupil_2</th>\n",
       "      <th>right_pupil_3</th>\n",
       "      <th>left_pupil_3</th>\n",
       "      <th>right_pupil_4</th>\n",
       "      <th>left_pupil_4</th>\n",
       "      <th>right_pupil_5</th>\n",
       "      <th>left_pupil_5</th>\n",
       "      <th>right_pupil_6</th>\n",
       "      <th>left_pupil_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.468321</td>\n",
       "      <td>0.467559</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.489414</td>\n",
       "      <td>0.479247</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.158416</td>\n",
       "      <td>0.147059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380298</td>\n",
       "      <td>0.444188</td>\n",
       "      <td>0.311828</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.172043</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.178947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.501544</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.468321</td>\n",
       "      <td>0.499871</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.134021</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.206186</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left_ear  right_ear  right_pupil_1  left_pupil_1  right_pupil_2  \\\n",
       "0  0.468321   0.467559       0.265306      0.220000       0.153061   \n",
       "1  0.489414   0.479247       0.257426      0.196078       0.148515   \n",
       "2  0.380298   0.444188       0.311828      0.284211       0.172043   \n",
       "3  0.501544   0.477273       0.315789      0.250000       0.189474   \n",
       "4  0.468321   0.499871       0.329897      0.270000       0.195876   \n",
       "\n",
       "   left_pupil_2  right_pupil_3  left_pupil_3  right_pupil_4  left_pupil_4  \\\n",
       "0      0.110000       0.102041      0.140000       0.204082      0.250000   \n",
       "1      0.088235       0.099010      0.137255       0.198020      0.264706   \n",
       "2      0.147368       0.064516      0.094737       0.161290      0.200000   \n",
       "3      0.140000       0.052632      0.090000       0.147368      0.210000   \n",
       "4      0.150000       0.051546      0.100000       0.134021      0.200000   \n",
       "\n",
       "   right_pupil_5  left_pupil_5  right_pupil_6  left_pupil_6  \n",
       "0       0.122449      0.140000       0.153061      0.140000  \n",
       "1       0.138614      0.166667       0.158416      0.147059  \n",
       "2       0.096774      0.094737       0.193548      0.178947  \n",
       "3       0.084211      0.110000       0.210526      0.200000  \n",
       "4       0.082474      0.100000       0.206186      0.180000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "151/151 [==============================] - 2s 6ms/step - loss: 153520.7596 - mse: 153517.7077 - val_loss: 46684.7773 - val_mse: 46682.3516\n",
      "Epoch 2/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 45227.3202 - mse: 45224.9813 - val_loss: 42137.1016 - val_mse: 42134.8320\n",
      "Epoch 3/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 42994.7868 - mse: 42992.5788 - val_loss: 42970.2305 - val_mse: 42968.0547\n",
      "Epoch 4/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 41721.1028 - mse: 41718.9271 - val_loss: 41118.7188 - val_mse: 41116.4766\n",
      "Epoch 5/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 27209.5650 - mse: 27206.6435 - val_loss: 16335.4473 - val_mse: 16331.5195\n",
      "Epoch 6/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 10656.0792 - mse: 10652.1536 - val_loss: 8726.2402 - val_mse: 8722.3203\n",
      "Epoch 7/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 8311.6443 - mse: 8307.7511 - val_loss: 5639.2041 - val_mse: 5635.2925\n",
      "Epoch 8/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 6965.4222 - mse: 6961.4985 - val_loss: 5389.0098 - val_mse: 5384.9961\n",
      "Epoch 9/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 6240.9909 - mse: 6236.7771 - val_loss: 4619.5122 - val_mse: 4614.7329\n",
      "Epoch 10/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 7078.9579 - mse: 7074.1553 - val_loss: 5345.9746 - val_mse: 5341.2246\n",
      "Epoch 11/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 6835.2323 - mse: 6830.4039 - val_loss: 5975.0459 - val_mse: 5970.2334\n",
      "Epoch 12/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 6419.0124 - mse: 6414.1448 - val_loss: 4508.2261 - val_mse: 4503.1611\n",
      "Epoch 13/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 6377.9874 - mse: 6372.9835 - val_loss: 5202.7505 - val_mse: 5197.6650\n",
      "Epoch 14/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 6220.5658 - mse: 6215.4287 - val_loss: 8161.2637 - val_mse: 8156.2422\n",
      "Epoch 15/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 6736.6904 - mse: 6731.5632 - val_loss: 6598.6660 - val_mse: 6593.2241\n",
      "Epoch 16/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 6577.0376 - mse: 6571.5134 - val_loss: 4366.0630 - val_mse: 4360.2749\n",
      "Epoch 17/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5952.8226 - mse: 5947.0246 - val_loss: 6756.1772 - val_mse: 6750.3994\n",
      "Epoch 18/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 5632.8351 - mse: 5627.1597 - val_loss: 4061.1555 - val_mse: 4055.2446\n",
      "Epoch 19/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5668.6188 - mse: 5662.7892 - val_loss: 4716.1128 - val_mse: 4710.2930\n",
      "Epoch 20/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 6473.6010 - mse: 6467.6663 - val_loss: 5964.0249 - val_mse: 5957.8823\n",
      "Epoch 21/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 6208.0301 - mse: 6201.8451 - val_loss: 4801.7739 - val_mse: 4795.3433\n",
      "Epoch 22/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5770.3633 - mse: 5763.9621 - val_loss: 4193.9927 - val_mse: 4187.5874\n",
      "Epoch 23/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5109.5649 - mse: 5103.1667 - val_loss: 4993.4028 - val_mse: 4987.0605\n",
      "Epoch 24/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5509.8431 - mse: 5503.4840 - val_loss: 4001.2314 - val_mse: 3994.8052\n",
      "Epoch 25/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5081.8916 - mse: 5075.4523 - val_loss: 7466.0796 - val_mse: 7459.5894\n",
      "Epoch 26/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5951.5182 - mse: 5944.9725 - val_loss: 5207.1118 - val_mse: 5200.3818\n",
      "Epoch 27/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 6209.3564 - mse: 6202.6780 - val_loss: 4712.5112 - val_mse: 4705.9131\n",
      "Epoch 28/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4887.7475 - mse: 4881.0721 - val_loss: 4487.6587 - val_mse: 4480.8384\n",
      "Epoch 29/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 6075.4349 - mse: 6068.6011 - val_loss: 3804.1904 - val_mse: 3797.1311\n",
      "Epoch 30/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4888.0959 - mse: 4881.0932 - val_loss: 5859.6460 - val_mse: 5852.6763\n",
      "Epoch 31/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 5166.0299 - mse: 5158.9785 - val_loss: 4126.2310 - val_mse: 4119.2378\n",
      "Epoch 32/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5345.2157 - mse: 5338.2342 - val_loss: 5427.8643 - val_mse: 5420.8418\n",
      "Epoch 33/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4760.3370 - mse: 4753.2687 - val_loss: 5921.4136 - val_mse: 5914.2344\n",
      "Epoch 34/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5072.6220 - mse: 5065.4254 - val_loss: 3944.2283 - val_mse: 3937.0503\n",
      "Epoch 35/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4864.7905 - mse: 4857.5916 - val_loss: 4430.6187 - val_mse: 4423.4697\n",
      "Epoch 36/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5199.9441 - mse: 5192.7713 - val_loss: 5538.3916 - val_mse: 5531.1865\n",
      "Epoch 37/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5054.0386 - mse: 5046.8265 - val_loss: 4392.1357 - val_mse: 4384.9175\n",
      "Epoch 38/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4748.0450 - mse: 4740.8368 - val_loss: 3820.0474 - val_mse: 3812.7505\n",
      "Epoch 39/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4353.3216 - mse: 4346.0051 - val_loss: 4079.8733 - val_mse: 4072.5476\n",
      "Epoch 40/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4618.8420 - mse: 4611.5011 - val_loss: 4748.9712 - val_mse: 4741.5767\n",
      "Epoch 41/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4728.1815 - mse: 4720.7917 - val_loss: 6129.9731 - val_mse: 6122.4858\n",
      "Epoch 42/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5426.3015 - mse: 5418.7871 - val_loss: 4380.9146 - val_mse: 4373.2588\n",
      "Epoch 43/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4819.9297 - mse: 4812.3539 - val_loss: 4391.0020 - val_mse: 4383.2749\n",
      "Epoch 44/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4677.2385 - mse: 4669.5525 - val_loss: 4315.6494 - val_mse: 4307.9458\n",
      "Epoch 45/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4771.0188 - mse: 4763.2205 - val_loss: 4993.1978 - val_mse: 4985.1982\n",
      "Epoch 46/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4409.0540 - mse: 4401.0470 - val_loss: 4149.6230 - val_mse: 4141.5449\n",
      "Epoch 47/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 5069.6881 - mse: 5061.6197 - val_loss: 4276.3833 - val_mse: 4268.2632\n",
      "Epoch 48/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4506.9450 - mse: 4498.8096 - val_loss: 4271.6118 - val_mse: 4263.3857\n",
      "Epoch 49/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4299.1728 - mse: 4290.9533 - val_loss: 6568.0898 - val_mse: 6559.9570\n",
      "Epoch 50/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4908.5342 - mse: 4900.3894 - val_loss: 4809.4033 - val_mse: 4801.0762\n",
      "Epoch 51/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4617.1386 - mse: 4608.7779 - val_loss: 3978.7483 - val_mse: 3970.3755\n",
      "Epoch 52/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4270.2493 - mse: 4261.8681 - val_loss: 8042.5557 - val_mse: 8033.9946\n",
      "Epoch 53/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 5111.5334 - mse: 5102.9738 - val_loss: 3984.8823 - val_mse: 3976.2651\n",
      "Epoch 54/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4761.1966 - mse: 4752.5017 - val_loss: 4533.3770 - val_mse: 4524.6733\n",
      "Epoch 55/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4956.0003 - mse: 4947.3094 - val_loss: 9888.3203 - val_mse: 9879.5518\n",
      "Epoch 56/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 5966.6387 - mse: 5957.7907 - val_loss: 3797.6335 - val_mse: 3788.6118\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 1s 4ms/step - loss: 4503.4360 - mse: 4494.3762 - val_loss: 4797.4404 - val_mse: 4788.3647\n",
      "Epoch 58/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5287.3781 - mse: 5278.1878 - val_loss: 4145.4966 - val_mse: 4136.2524\n",
      "Epoch 59/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4495.7208 - mse: 4486.4452 - val_loss: 6307.8164 - val_mse: 6298.4448\n",
      "Epoch 60/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4451.0800 - mse: 4441.6748 - val_loss: 4463.4531 - val_mse: 4453.9370\n",
      "Epoch 61/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4500.4809 - mse: 4490.9038 - val_loss: 6030.5337 - val_mse: 6020.8145\n",
      "Epoch 62/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4616.7073 - mse: 4606.9155 - val_loss: 4664.3599 - val_mse: 4654.5571\n",
      "Epoch 63/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4570.2096 - mse: 4560.3420 - val_loss: 4402.3413 - val_mse: 4392.3848\n",
      "Epoch 64/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4589.6768 - mse: 4579.6912 - val_loss: 4176.7554 - val_mse: 4166.6831\n",
      "Epoch 65/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4544.9124 - mse: 4534.8472 - val_loss: 6014.4395 - val_mse: 6004.2178\n",
      "Epoch 66/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4895.1112 - mse: 4884.9382 - val_loss: 3929.2085 - val_mse: 3919.0835\n",
      "Epoch 67/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4338.7740 - mse: 4328.6558 - val_loss: 3788.8582 - val_mse: 3778.7668\n",
      "Epoch 68/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4527.8511 - mse: 4517.7531 - val_loss: 4313.8931 - val_mse: 4303.7114\n",
      "Epoch 69/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4163.4811 - mse: 4153.2609 - val_loss: 4456.5259 - val_mse: 4446.2397\n",
      "Epoch 70/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 5012.6105 - mse: 5002.3027 - val_loss: 4237.8579 - val_mse: 4227.3794\n",
      "Epoch 71/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4448.7538 - mse: 4438.1869 - val_loss: 4062.2061 - val_mse: 4051.6118\n",
      "Epoch 72/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4091.4005 - mse: 4080.7931 - val_loss: 5764.9136 - val_mse: 5754.3105\n",
      "Epoch 73/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4314.9582 - mse: 4304.2553 - val_loss: 4456.0557 - val_mse: 4445.4155\n",
      "Epoch 74/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4060.8260 - mse: 4050.1780 - val_loss: 4020.0994 - val_mse: 4009.3640\n",
      "Epoch 75/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4512.6108 - mse: 4501.8607 - val_loss: 5887.5483 - val_mse: 5876.6260\n",
      "Epoch 76/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4226.0382 - mse: 4215.0920 - val_loss: 4893.8613 - val_mse: 4882.8384\n",
      "Epoch 77/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4220.4846 - mse: 4209.4238 - val_loss: 6451.4829 - val_mse: 6440.4541\n",
      "Epoch 78/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4131.8075 - mse: 4120.7599 - val_loss: 3763.7173 - val_mse: 3752.5632\n",
      "Epoch 79/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4731.1490 - mse: 4719.9913 - val_loss: 4092.2004 - val_mse: 4081.0586\n",
      "Epoch 80/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4138.3799 - mse: 4127.2209 - val_loss: 3818.0435 - val_mse: 3806.7881\n",
      "Epoch 81/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4103.3697 - mse: 4092.1081 - val_loss: 4813.1348 - val_mse: 4801.9121\n",
      "Epoch 82/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4496.7380 - mse: 4485.4433 - val_loss: 3861.8152 - val_mse: 3850.4368\n",
      "Epoch 83/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4167.9345 - mse: 4156.5169 - val_loss: 4166.8770 - val_mse: 4155.4102\n",
      "Epoch 84/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4394.3525 - mse: 4382.9097 - val_loss: 4304.2349 - val_mse: 4292.7744\n",
      "Epoch 85/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4123.6092 - mse: 4112.1092 - val_loss: 3728.9890 - val_mse: 3717.3286\n",
      "Epoch 86/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4033.6255 - mse: 4021.9133 - val_loss: 4334.9131 - val_mse: 4323.1816\n",
      "Epoch 87/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4055.2850 - mse: 4043.6007 - val_loss: 3817.6487 - val_mse: 3806.0547\n",
      "Epoch 88/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 3868.7828 - mse: 3857.1564 - val_loss: 4321.5117 - val_mse: 4309.8325\n",
      "Epoch 89/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4251.5266 - mse: 4239.7905 - val_loss: 4031.4568 - val_mse: 4019.6379\n",
      "Epoch 90/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3965.1721 - mse: 3953.3336 - val_loss: 3838.1531 - val_mse: 3826.1716\n",
      "Epoch 91/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4990.4284 - mse: 4978.3560 - val_loss: 4073.2180 - val_mse: 4060.9888\n",
      "Epoch 92/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4101.3347 - mse: 4089.1179 - val_loss: 4494.8564 - val_mse: 4482.6431\n",
      "Epoch 93/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4189.6692 - mse: 4177.3648 - val_loss: 4142.1270 - val_mse: 4129.7603\n",
      "Epoch 94/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4132.3360 - mse: 4119.8831 - val_loss: 4431.1118 - val_mse: 4418.4961\n",
      "Epoch 95/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3910.4778 - mse: 3897.8056 - val_loss: 4164.9106 - val_mse: 4152.2705\n",
      "Epoch 96/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4269.6358 - mse: 4256.9845 - val_loss: 4731.4360 - val_mse: 4718.6377\n",
      "Epoch 97/200\n",
      "151/151 [==============================] - 1s 6ms/step - loss: 4140.1126 - mse: 4127.3908 - val_loss: 3975.8208 - val_mse: 3963.2356\n",
      "Epoch 98/200\n",
      "151/151 [==============================] - 1s 6ms/step - loss: 3887.6463 - mse: 3875.0516 - val_loss: 3693.9436 - val_mse: 3681.0862\n",
      "Epoch 99/200\n",
      "151/151 [==============================] - 1s 6ms/step - loss: 4104.2096 - mse: 4091.3214 - val_loss: 4859.0059 - val_mse: 4846.1753\n",
      "Epoch 100/200\n",
      "151/151 [==============================] - 1s 6ms/step - loss: 4422.3382 - mse: 4409.4903 - val_loss: 4381.7676 - val_mse: 4368.9092\n",
      "Epoch 101/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 4003.1012 - mse: 3990.2274 - val_loss: 4395.3213 - val_mse: 4382.4023\n",
      "Epoch 102/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3907.9603 - mse: 3895.0443 - val_loss: 3532.1365 - val_mse: 3519.1064\n",
      "Epoch 103/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4006.3976 - mse: 3993.3411 - val_loss: 6372.0220 - val_mse: 6358.7754\n",
      "Epoch 104/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4812.4857 - mse: 4799.1987 - val_loss: 4313.8525 - val_mse: 4300.4731\n",
      "Epoch 105/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 3967.6791 - mse: 3954.3141 - val_loss: 4404.7119 - val_mse: 4391.2285\n",
      "Epoch 106/200\n",
      "151/151 [==============================] - 1s 6ms/step - loss: 4124.2560 - mse: 4110.7370 - val_loss: 4868.8315 - val_mse: 4855.2310\n",
      "Epoch 107/200\n",
      "151/151 [==============================] - 1s 6ms/step - loss: 3929.7756 - mse: 3916.1362 - val_loss: 3922.0510 - val_mse: 3908.4182\n",
      "Epoch 108/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 3766.5025 - mse: 3752.8197 - val_loss: 3987.0842 - val_mse: 3973.3098\n",
      "Epoch 109/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 3801.6540 - mse: 3787.8778 - val_loss: 3677.1057 - val_mse: 3663.3579\n",
      "Epoch 110/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 3810.3712 - mse: 3796.6134 - val_loss: 3762.4727 - val_mse: 3748.7896\n",
      "Epoch 111/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 3880.4599 - mse: 3866.7516 - val_loss: 4334.0107 - val_mse: 4320.1890\n",
      "Epoch 112/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 3823.7609 - mse: 3809.8662 - val_loss: 4818.3711 - val_mse: 4804.2339\n",
      "Epoch 113/200\n",
      "151/151 [==============================] - 1s 5ms/step - loss: 3996.6012 - mse: 3982.4591 - val_loss: 4138.0840 - val_mse: 4123.7646\n",
      "Epoch 114/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4005.8223 - mse: 3991.5371 - val_loss: 3743.3406 - val_mse: 3728.9849\n",
      "Epoch 115/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4006.0385 - mse: 3991.6558 - val_loss: 4121.2227 - val_mse: 4106.8213\n",
      "Epoch 116/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3756.1877 - mse: 3741.7792 - val_loss: 4985.5415 - val_mse: 4971.0332\n",
      "Epoch 117/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4290.1561 - mse: 4275.6110 - val_loss: 3978.9639 - val_mse: 3964.2192\n",
      "Epoch 118/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4048.1840 - mse: 4033.4507 - val_loss: 5327.8730 - val_mse: 5313.2832\n",
      "Epoch 119/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4248.4034 - mse: 4233.7531 - val_loss: 4479.5381 - val_mse: 4464.8896\n",
      "Epoch 120/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3985.0818 - mse: 3970.3186 - val_loss: 3945.1521 - val_mse: 3930.1714\n",
      "Epoch 121/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3743.8749 - mse: 3728.9136 - val_loss: 3723.4380 - val_mse: 3708.5430\n",
      "Epoch 122/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3821.4524 - mse: 3806.5213 - val_loss: 3549.4758 - val_mse: 3534.3938\n",
      "Epoch 123/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3685.1200 - mse: 3670.0565 - val_loss: 3815.8286 - val_mse: 3800.7544\n",
      "Epoch 124/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4294.0562 - mse: 4278.9213 - val_loss: 3670.5752 - val_mse: 3655.4548\n",
      "Epoch 125/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3893.3372 - mse: 3878.1766 - val_loss: 3809.9829 - val_mse: 3794.6562\n",
      "Epoch 126/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3767.1289 - mse: 3751.7923 - val_loss: 4687.9517 - val_mse: 4672.5366\n",
      "Epoch 127/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3958.0955 - mse: 3942.7060 - val_loss: 3682.9263 - val_mse: 3667.5752\n",
      "Epoch 128/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3474.9347 - mse: 3459.6314 - val_loss: 3664.5967 - val_mse: 3649.1270\n",
      "Epoch 129/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3980.7092 - mse: 3965.2242 - val_loss: 3956.2383 - val_mse: 3940.8762\n",
      "Epoch 130/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3780.5164 - mse: 3765.1552 - val_loss: 3725.9978 - val_mse: 3710.5469\n",
      "Epoch 131/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3962.5501 - mse: 3947.0463 - val_loss: 3716.6753 - val_mse: 3701.0098\n",
      "Epoch 132/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3817.5523 - mse: 3801.8828 - val_loss: 4063.2222 - val_mse: 4047.5249\n",
      "Epoch 133/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3880.1946 - mse: 3864.4715 - val_loss: 3824.4570 - val_mse: 3808.8093\n",
      "Epoch 134/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4275.0215 - mse: 4259.3068 - val_loss: 3808.7952 - val_mse: 3792.8003\n",
      "Epoch 135/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3584.3807 - mse: 3568.3912 - val_loss: 3724.9985 - val_mse: 3709.0166\n",
      "Epoch 136/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3540.3689 - mse: 3524.3957 - val_loss: 4765.8501 - val_mse: 4749.9004\n",
      "Epoch 137/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3911.6127 - mse: 3895.5956 - val_loss: 3600.7263 - val_mse: 3584.7312\n",
      "Epoch 138/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3540.1807 - mse: 3524.1724 - val_loss: 4437.1660 - val_mse: 4421.0522\n",
      "Epoch 139/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3618.3734 - mse: 3602.2831 - val_loss: 4552.2124 - val_mse: 4536.0361\n",
      "Epoch 140/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3780.2014 - mse: 3764.0825 - val_loss: 3719.9097 - val_mse: 3703.7922\n",
      "Epoch 141/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3830.3385 - mse: 3814.1814 - val_loss: 3898.8979 - val_mse: 3882.7227\n",
      "Epoch 142/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3866.3451 - mse: 3850.1423 - val_loss: 4363.6318 - val_mse: 4347.3130\n",
      "Epoch 143/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3848.7130 - mse: 3832.3284 - val_loss: 3763.7124 - val_mse: 3747.3496\n",
      "Epoch 144/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3483.7776 - mse: 3467.4012 - val_loss: 3606.2820 - val_mse: 3589.8030\n",
      "Epoch 145/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3700.6820 - mse: 3684.2586 - val_loss: 3654.1013 - val_mse: 3637.5430\n",
      "Epoch 146/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3689.8821 - mse: 3673.2926 - val_loss: 3743.3188 - val_mse: 3726.6155\n",
      "Epoch 147/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3678.7565 - mse: 3662.0128 - val_loss: 3881.6716 - val_mse: 3864.8118\n",
      "Epoch 148/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3984.9656 - mse: 3968.0597 - val_loss: 3860.7502 - val_mse: 3843.8511\n",
      "Epoch 149/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3934.7139 - mse: 3917.7430 - val_loss: 4391.2524 - val_mse: 4374.2339\n",
      "Epoch 150/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3658.2374 - mse: 3641.1818 - val_loss: 4014.4888 - val_mse: 3997.4026\n",
      "Epoch 151/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3432.9032 - mse: 3415.8309 - val_loss: 3828.5786 - val_mse: 3811.4822\n",
      "Epoch 152/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3665.3124 - mse: 3648.1823 - val_loss: 4718.6875 - val_mse: 4701.5596\n",
      "Epoch 153/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3538.0416 - mse: 3520.8821 - val_loss: 4514.1763 - val_mse: 4497.0571\n",
      "Epoch 154/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3716.1676 - mse: 3699.0378 - val_loss: 3836.5376 - val_mse: 3819.2202\n",
      "Epoch 155/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3633.0613 - mse: 3615.7692 - val_loss: 3635.5598 - val_mse: 3618.2175\n",
      "Epoch 156/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3634.4614 - mse: 3617.1744 - val_loss: 3684.7146 - val_mse: 3667.4814\n",
      "Epoch 157/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3839.9654 - mse: 3822.6454 - val_loss: 3618.4956 - val_mse: 3601.0134\n",
      "Epoch 158/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3698.7933 - mse: 3681.3217 - val_loss: 3876.1973 - val_mse: 3858.6382\n",
      "Epoch 159/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3480.3131 - mse: 3462.6826 - val_loss: 3625.0503 - val_mse: 3607.3027\n",
      "Epoch 160/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3470.3466 - mse: 3452.6037 - val_loss: 4381.7402 - val_mse: 4363.9956\n",
      "Epoch 161/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3786.5314 - mse: 3768.8222 - val_loss: 4707.3989 - val_mse: 4689.7031\n",
      "Epoch 162/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3600.0571 - mse: 3582.3630 - val_loss: 4459.4697 - val_mse: 4441.8926\n",
      "Epoch 163/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3868.7547 - mse: 3851.0909 - val_loss: 4095.5042 - val_mse: 4077.7346\n",
      "Epoch 164/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3521.5678 - mse: 3503.7901 - val_loss: 4173.5859 - val_mse: 4155.8218\n",
      "Epoch 165/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3507.1633 - mse: 3489.3094 - val_loss: 4257.2808 - val_mse: 4239.4170\n",
      "Epoch 166/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3659.1304 - mse: 3641.2418 - val_loss: 4337.5112 - val_mse: 4319.4805\n",
      "Epoch 167/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 4072.0759 - mse: 4053.9863 - val_loss: 3632.3389 - val_mse: 3614.2703\n",
      "Epoch 168/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3651.7380 - mse: 3633.6369 - val_loss: 3891.9036 - val_mse: 3873.8179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3576.1132 - mse: 3558.0726 - val_loss: 4040.4990 - val_mse: 4022.4578\n",
      "Epoch 170/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3618.7899 - mse: 3600.7307 - val_loss: 4436.5122 - val_mse: 4418.3394\n",
      "Epoch 171/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3683.4941 - mse: 3665.3201 - val_loss: 3649.6211 - val_mse: 3631.2866\n",
      "Epoch 172/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3471.4486 - mse: 3453.0727 - val_loss: 4683.2471 - val_mse: 4664.7090\n",
      "Epoch 173/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3752.4836 - mse: 3733.8724 - val_loss: 4212.8149 - val_mse: 4194.0552\n",
      "Epoch 174/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3560.8275 - mse: 3542.0698 - val_loss: 3583.8000 - val_mse: 3565.0068\n",
      "Epoch 175/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3407.1116 - mse: 3388.3046 - val_loss: 4050.4600 - val_mse: 4031.5215\n",
      "Epoch 176/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3564.3193 - mse: 3545.3963 - val_loss: 5266.1909 - val_mse: 5247.2183\n",
      "Epoch 177/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3501.4419 - mse: 3482.4899 - val_loss: 3831.9919 - val_mse: 3812.9539\n",
      "Epoch 178/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3626.1054 - mse: 3607.0083 - val_loss: 4029.5386 - val_mse: 4010.3992\n",
      "Epoch 179/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3527.3119 - mse: 3508.1451 - val_loss: 3880.2644 - val_mse: 3861.0837\n",
      "Epoch 180/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3457.5173 - mse: 3438.3057 - val_loss: 4005.2622 - val_mse: 3986.0669\n",
      "Epoch 181/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3738.6360 - mse: 3719.3255 - val_loss: 3941.4150 - val_mse: 3921.9683\n",
      "Epoch 182/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3566.4665 - mse: 3546.9953 - val_loss: 4245.8379 - val_mse: 4226.0303\n",
      "Epoch 183/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3510.2119 - mse: 3490.3330 - val_loss: 4007.2566 - val_mse: 3987.3845\n",
      "Epoch 184/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3541.3222 - mse: 3521.4873 - val_loss: 3487.5354 - val_mse: 3467.8625\n",
      "Epoch 185/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3587.2089 - mse: 3567.5116 - val_loss: 4505.3003 - val_mse: 4485.7070\n",
      "Epoch 186/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3566.0533 - mse: 3546.3981 - val_loss: 3501.9839 - val_mse: 3482.2773\n",
      "Epoch 187/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3741.3713 - mse: 3721.6512 - val_loss: 3691.2505 - val_mse: 3671.5559\n",
      "Epoch 188/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3531.2692 - mse: 3511.5286 - val_loss: 4798.7920 - val_mse: 4778.9751\n",
      "Epoch 189/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3639.7165 - mse: 3619.8302 - val_loss: 3556.0374 - val_mse: 3536.1152\n",
      "Epoch 190/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3374.6448 - mse: 3354.7424 - val_loss: 3574.9209 - val_mse: 3554.9517\n",
      "Epoch 191/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3617.5706 - mse: 3597.5419 - val_loss: 3448.1255 - val_mse: 3428.0532\n",
      "Epoch 192/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3546.7032 - mse: 3526.6313 - val_loss: 4858.1909 - val_mse: 4838.0259\n",
      "Epoch 193/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3655.2890 - mse: 3635.1417 - val_loss: 3895.2756 - val_mse: 3875.0359\n",
      "Epoch 194/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3846.6328 - mse: 3826.3753 - val_loss: 4625.3481 - val_mse: 4605.0679\n",
      "Epoch 195/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3752.4958 - mse: 3732.1612 - val_loss: 4374.2637 - val_mse: 4353.8052\n",
      "Epoch 196/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3670.4200 - mse: 3649.9769 - val_loss: 4139.6860 - val_mse: 4119.2930\n",
      "Epoch 197/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3410.3673 - mse: 3389.9733 - val_loss: 3980.0869 - val_mse: 3959.6477\n",
      "Epoch 198/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3433.1819 - mse: 3412.6711 - val_loss: 3527.3821 - val_mse: 3506.8811\n",
      "Epoch 199/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3450.9255 - mse: 3430.4357 - val_loss: 5165.0815 - val_mse: 5144.5312\n",
      "Epoch 200/200\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 3722.6569 - mse: 3702.0854 - val_loss: 6056.6934 - val_mse: 6036.1143\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, validation_split=0.25, epochs =200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d60c17490>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_ear</th>\n",
       "      <th>right_ear</th>\n",
       "      <th>right_pupil_1</th>\n",
       "      <th>left_pupil_1</th>\n",
       "      <th>right_pupil_2</th>\n",
       "      <th>left_pupil_2</th>\n",
       "      <th>right_pupil_3</th>\n",
       "      <th>left_pupil_3</th>\n",
       "      <th>right_pupil_4</th>\n",
       "      <th>left_pupil_4</th>\n",
       "      <th>right_pupil_5</th>\n",
       "      <th>left_pupil_5</th>\n",
       "      <th>right_pupil_6</th>\n",
       "      <th>left_pupil_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.219492</td>\n",
       "      <td>0.193596</td>\n",
       "      <td>0.314607</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.210145</td>\n",
       "      <td>0.089888</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.258427</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.246377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.176589</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.201550</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.085271</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.217054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0.290396</td>\n",
       "      <td>0.373440</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.314607</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.213483</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.122222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0.290396</td>\n",
       "      <td>0.350190</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.215909</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.144444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.288604</td>\n",
       "      <td>0.366372</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.138298</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.180851</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.223404</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>0.074468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>0.278108</td>\n",
       "      <td>0.308985</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.221154</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.139785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>0.282542</td>\n",
       "      <td>0.275588</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.195652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>0.260808</td>\n",
       "      <td>0.264003</td>\n",
       "      <td>0.316327</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.112245</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.112245</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.208791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>0.288818</td>\n",
       "      <td>0.313679</td>\n",
       "      <td>0.349515</td>\n",
       "      <td>0.318681</td>\n",
       "      <td>0.165049</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.077670</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.155340</td>\n",
       "      <td>0.197802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.296674</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.183908</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.218391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1232 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      left_ear  right_ear  right_pupil_1  left_pupil_1  right_pupil_2  \\\n",
       "43    0.219492   0.193596       0.314607      0.311594       0.101124   \n",
       "154   0.176589   0.191489       0.282609      0.333333       0.086957   \n",
       "747   0.290396   0.373440       0.179775      0.233333       0.056180   \n",
       "752   0.290396   0.350190       0.181818      0.300000       0.045455   \n",
       "767   0.288604   0.366372       0.161290      0.138298       0.053763   \n",
       "...        ...        ...            ...           ...            ...   \n",
       "1927  0.278108   0.308985       0.307692      0.268817       0.125000   \n",
       "1928  0.282542   0.275588       0.310000      0.304348       0.130000   \n",
       "1932  0.260808   0.264003       0.316327      0.329670       0.112245   \n",
       "1987  0.288818   0.313679       0.349515      0.318681       0.165049   \n",
       "1990  0.295455   0.296674       0.353535      0.321839       0.161616   \n",
       "\n",
       "      left_pupil_2  right_pupil_3  left_pupil_3  right_pupil_4  left_pupil_4  \\\n",
       "43        0.210145       0.089888      0.086957       0.258427      0.028986   \n",
       "154       0.201550       0.108696      0.093023       0.293478      0.069767   \n",
       "747       0.100000       0.179775      0.133333       0.314607      0.277778   \n",
       "752       0.166667       0.181818      0.088889       0.318182      0.233333   \n",
       "767       0.021277       0.193548      0.180851       0.322581      0.361702   \n",
       "...            ...            ...           ...            ...           ...   \n",
       "1927      0.118280       0.096154      0.096774       0.221154      0.258065   \n",
       "1928      0.163043       0.090000      0.054348       0.240000      0.206522   \n",
       "1932      0.186813       0.091837      0.043956       0.244898      0.186813   \n",
       "1987      0.175824       0.058252      0.054945       0.194175      0.186813   \n",
       "1990      0.172414       0.050505      0.045977       0.191919      0.183908   \n",
       "\n",
       "      right_pupil_5  left_pupil_5  right_pupil_6  left_pupil_6  \n",
       "43         0.112360      0.115942       0.123596      0.246377  \n",
       "154        0.130435      0.085271       0.097826      0.217054  \n",
       "747        0.213483      0.133333       0.056180      0.122222  \n",
       "752        0.215909      0.066667       0.056818      0.144444  \n",
       "767        0.225806      0.223404       0.043011      0.074468  \n",
       "...             ...           ...            ...           ...  \n",
       "1927       0.115385      0.118280       0.134615      0.139785  \n",
       "1928       0.110000      0.076087       0.120000      0.195652  \n",
       "1932       0.112245      0.043956       0.122449      0.208791  \n",
       "1987       0.077670      0.065934       0.155340      0.197802  \n",
       "1990       0.070707      0.057471       0.171717      0.218391  \n",
       "\n",
       "[1232 rows x 14 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"left_ear\"] <= 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(Y_preds, Y, n) :\n",
    "    acc = np.sum(np.sqrt(np.sum(np.power(Y_x - Y_preds, 2), axis = 1))) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict(X_test)\n",
    "# for i in range(preds.shape[0]):\n",
    "#     print(preds[i])\n",
    "#     print(Y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/model10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "metadata[\"columns\"] = train_columns.tolist()\n",
    "\n",
    "\n",
    "file = open(\"models/model10.data\", \"+w\")\n",
    "file.write(json.dumps(metadata))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
